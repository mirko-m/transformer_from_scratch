# About
This repo is the result of my following along with Andrej Karpathy's video
lecture on building GPT from scratch [1]. It contains code for a transformer,
specifically a decoder only model which is very similar to the original
formulation by Vaswani et al. [2]. The main differences to the original
formulationa are:

- The architecture being a decoder only model.
- The use of learnable positional encodings.
- The use of the so called pre-layer-norm variation [3].

Furthermore, the model uses individual characters as tokens instead of using
word tokens or a more sophisticated tookenization method such as byte pair
encoding.

# Data
I use two datasets. One of them is the Shakespeare dataset which Karpathy uses
in his video lecture. Unsurprisingly it contains plays from William Shakespeare.
The second datataset consists of _Oliver Twist_ and _Great Expectations_ by
Charles Dickens and was taken from Project Gutenberg [4].

# Training
I used torch 1.13.0 and an Nvidia RTX A5000 Laptop GPU. There is a python script
called `run.py` which can be used for both training and prediction. To train a
model on the Shakespeare dataset just do
```bash
./run.py  --input shakespeare.txt
```
During training the first 90% of the dataset will be used for training and the
remaining 10% will be used for validation. You can control the split using the
`train_frac` parameter of the dataloader. The best model checkpoint will be
saved to a file called `model_checkpoint_<timestamp>.pt`. Below is an example of
what the output from running the training script looks like.
```bash
Training on device: cuda
i:      0, train_loss: 4.35, val_loss: 3.62
i:    500, train_loss: 2.07, val_loss: 2.09
i:   1000, train_loss: 1.69, val_loss: 1.79
i:   1500, train_loss: 1.51, val_loss: 1.66
i:   2000, train_loss: 1.42, val_loss: 1.59
i:   2500, train_loss: 1.35, val_loss: 1.56
i:   3000, train_loss: 1.30, val_loss: 1.52
i:   3500, train_loss: 1.26, val_loss: 1.52
i:   4000, train_loss: 1.22, val_loss: 1.52
i:   4500, train_loss: 1.19, val_loss: 1.52
Training finished in 1586.555766424s.
```
As you can see the validation loss plateaus after about 3000 iterations and the
training took about 26 minutes.

# Some results
After training a model you can generate text by doing
```bash
./run.py --generate --input shakespeare.txt --checkpoint model_checkpoint_<timestamp>.pt
```

Below are some results i.e. completions for models trained on the Shakespeare
and Dickens datasets respectively. In both cases the prompt is shown as normal
text and the text generated by the model is shown in bold. Note that when you
run the model yourself the symbol  `<|>` is used to separate the prompt from the
generated text when printing to stdout. Also, remember that we use individual
characters as tokens, so it is possible for the model to generate words which do
not exist.

We can see that the model does learn to produce text that looks similar to the
training data, but the generated text is not coherent. In other words, it is
gibberish. Furthermore, some of the words the model generates do not exist. For
example `commiation` appears in the generated output for the Dickens model.

It would be interesting to compare these results to an N-Gram model.  Peter
Norvigs' pytudes contain a notebook which shows that N-Gram models can also do
quite well at generating gibberish that superficially looks like the training
data, but they are of course not as good at capturing long term dependencies as
transformers [5].
## Shakespeare
<pre>
?

GREMIO:
Good morrow, neighbour Baptista.

BAPTISTA:
Good morrow, neighbour Gremio.
God save you, gentlemen!

PETRUCHIO:
And you, good sir! Pray, have you not a daughter
Call'd Katharina, fair and virtuous?

BAPTISTA:
I have a daughter, sir, called Katha<b>rina,
And that wash'd the racket book in Caget,
Against the devil widow of day's bulk.

CORIOLANUS:
And then keep him that Angelo be bound;
When were we are our lords for woman's speak:
What we used us to do?

MENENIUS:
Give me, and friar
How I member than my head; what duty
To his occasion, is the mother
Than hours, may be else I send, and in
With the corruption of Hereford!
From them confer my negligences,
My father words my quittiet or thing,
Which I both written to be care:
And fall we are to proceed.
Salemned that the heavens with him,
When sour that replies, bless
The oath hour nor wife orname is but now,
Horten so much in his presently,
To-more night, and Leontes Summan,
Any Coriolanus, my brother, Norfolk,
I will may break our hands: ay,
I know the credition judgment o' the sacred state
Of a five opetion, and bid the homour of excels
In petting, if with what they presently,
They say their time, peruship; here I
They earth thou desired?

BUSHY:
They will revenge you in thy realm </b>
</pre>

## Dickens
<pre>
 to me to-night?”

“Nothing. I got your letter and destroyed it. Nothing.”

We exchanged a cordial good-night, and I went home, with new matter for
my thoughts, though with no relief from the old.




Chapter XLIX.


Putting Miss Havisham’s note in my pock<b>ets and scarcely, but we began
to his commiation and conversive that Mr. Wemmick would be started in front
his condition to him to a Commiliary Bolley had disturbed his own gloomy.

There was a starting window,—much moving such creatures by the
great-up of good favourities, when the man was until he had come into the ground.

“The road! He being varied of you everybody?” was a lady dog.

“Ho Mr. Pocket’s you, sir,” cried Joe, skiffering himself to see and told
her blood, “over the means usually things, with
you?”

“A boy as were letter.”

“Well!” he said, looking Bates. “When you thought yourself, Pip,” said
the black. “We wouldn’t you, that’s eating, Joe, who beckoned the parish such table
picks and would be entertained its fleast.”

“God bless yourself you, Pieces, you’re of lady’s parried in her, a
person of caused not,” said I, “and with walk.”

“It’s very here the girl down?”

“If I am afraid,” remonstrated the Jew, paying the infant, “that I’ve
less and different, somebody, and t</b>
</pre>
# Useful References
1. Andrej Karpathy's video lecture on building GPT from scratch: https://www.youtube.com/watch?v=kCc8FmEb1nY
2. Attention is All You Need, Vaswani et al. (2017): https://arxiv.org/abs/1706.03762
3. On Layer Normalization in the Transformer Architecture, Xiong et al. (2020): https://arxiv.org/abs/2002.04745
4. Charles Dickens's books on Project Gutenberg: https://www.gutenberg.org/ebooks/author/37
5. Peter Norvig's jupyter notebook: https://github.com/norvig/pytudes/blob/main/ipynb/Goldberg.ipynb